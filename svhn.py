# -*- coding: utf-8 -*-
"""svhn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cr_mqeEfw7how-r9MAqqZqJqyepX31yS
"""

import numpy as np
import matplotlib.pyplot as plt
#import seaborn as sns
import h5py
#import tensorflow as tf
#import os
#import time
#import math
#from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
#from datetime import timedelta

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation
from keras.layers.normalization import BatchNormalization
#from keras.preprocessing import image

from keras.optimizers import adam
from tensorflow import keras

plt.rcParams['figure.figsize'] = (16.0, 4.0)

import urllib.request
from scipy.io import loadmat

from sklearn.preprocessing import OneHotEncoder

# Dataset loading
def load_data(path):
    """ Helper function for loading a MAT-File"""
    data = loadmat(path)
    return data['X'], data['y']

def balanced_subsample(y, s):
    """Return a balanced subsample of the population"""
    sample = []
    # For every label in the dataset
    for label in np.unique(y):
        # Get the index of all images with a specific label
        images = np.where(y==label)[0]
        # Draw a random sample from the images
        random_sample = np.random.choice(images, size=s, replace=False)
        # Add the random sample to our subsample list
        sample += random_sample.tolist()
    return sample

def rgb2gray(images):
    """Convert images from rbg to grayscale
    """
    return np.expand_dims(np.dot(images, [0.2989, 0.5870, 0.1140]), axis=3)
  
def downloadset():
    urllib.request.urlretrieve("http://ufldl.stanford.edu/housenumbers/train_32x32.mat", "train_32x32.mat")
    urllib.request.urlretrieve("http://ufldl.stanford.edu/housenumbers/test_32x32.mat", "test_32x32.mat")

def preprocessing():
    # Downloading
    downloadset()
    #urllib.request.urlretrieve("http://ufldl.stanford.edu/housenumbers/train_32x32.mat", "data/train_32x32.mat")
    #urllib.request.urlretrieve("http://ufldl.stanford.edu/housenumbers/test_32x32.mat", "data/test_32x32.mat")
    
    #Loading
    X_train, y_train = load_data('train_32x32.mat')
    X_test, y_test = load_data('test_32x32.mat')
    
    # Transpose the image arrays
    X_train, y_train = X_train.transpose((3,0,1,2)), y_train[:,0]
    X_test, y_test = X_test.transpose((3,0,1,2)), y_test[:,0]
    
    # Change labels
    y_train[y_train == 10] = 0
    y_test[y_test == 10] = 0
    
    train_samples = balanced_subsample(y_train, 600)
    
    X_val, y_val = np.copy(X_train[train_samples]), np.copy(y_train[train_samples])

    # Remove the samples to avoid duplicates
    X_train = np.delete(X_train, train_samples, axis=0)
    y_train = np.delete(y_train, train_samples, axis=0)
    
    X_test, y_test = X_test, y_test
    
    # Assert that we did not remove or add any duplicates
    # assert(num_images == X_train.shape[0] + X_test.shape[0] + X_val.shape[0])
    
    # Transform the images to greyscale
    train_greyscale = rgb2gray(X_train).astype(np.float32)
    test_greyscale = rgb2gray(X_test).astype(np.float32)
    valid_greyscale = rgb2gray(X_val).astype(np.float32)
    
    # Fit the OneHotEncoder
    enc = OneHotEncoder().fit(y_train.reshape(-1, 1))

    # Transform the label values to a one-hot-encoding scheme
    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()
    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()
    y_val = enc.transform(y_val.reshape(-1, 1)).toarray()
    
    

    # Create file
    h5f = h5py.File('SVHN_single_grey.h5', 'w')

    # Store the datasets
    h5f.create_dataset('X_train', data=train_greyscale)
    h5f.create_dataset('y_train', data=y_train)
    h5f.create_dataset('X_test', data=test_greyscale)
    h5f.create_dataset('y_test', data=y_test)
    h5f.create_dataset('X_val', data=valid_greyscale)
    h5f.create_dataset('y_val', data=y_val)

    # Close the file
    h5f.close()

def build_model(input_shape=(32, 32, 1)):
    model = Sequential()
    model.add(Conv2D(32, kernel_size=3, input_shape=input_shape, padding="same"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(32, 3, padding="same"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(0.3))

    model.add(Conv2D(64, 3))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(64, 3, padding="same"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(0.3))

    model.add(Conv2D(128, 3))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Conv2D(128, 3, padding="same"))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Dropout(0.3))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.3))

    model.add(Dense(10, activation='softmax'))
    
    model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=adam(lr=0.001, decay=1e-6),
              metrics=['accuracy'])
    return model

def train_model(x_train, y_train, x_val, y_val):
    
    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='max')
    
    model = build_model()
    model.fit(x_train, y_train,
          batch_size=64,
          epochs=50,
          verbose=1,
          validation_data=(x_val, y_val),
          callbacks=[early_stopping])
    return model

def traintest():
    preprocessing()
    # Open the file as readonly
    h5f = h5py.File('SVHN_single_grey.h5', 'r')

    # Load the training, test and validation set
    X_train = h5f['X_train'][:]
    y_train = h5f['y_train'][:]
    X_test = h5f['X_test'][:]
    y_test = h5f['y_test'][:]
    X_val = h5f['X_val'][:]
    y_val = h5f['y_val'][:]

    # Close this file
    h5f.close()
    
    cnnmodel = train_model(X_train, y_train, X_val, y_val)
    
    score = cnnmodel.evaluate(X_test, y_test, verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    y_ = np.array([1,2,3,4,5,6,7,8,9,0])
    enc = OneHotEncoder().fit(y_.reshape(-1, 1))
    y_pred = cnnmodel.predict_classes(X_test, batch_size=32, verbose=1)
    y_pred = enc.transform(y_pred.reshape(-1, 1)).toarray()
    
    f1 = f1_score(y_test, y_pred, average = None)  # return an array of F1_score for each layer
    output_string = ""
    for i in range(0, len(f1)):
        output_string += "F1 score(target = {}): {}".format(i, f1[i]) # Print each class respectively
        output_string += '\n'
    #print ('f1score:', f1)
    cnnmodel.save("svhn.h5")
    return output_string

from keras.models import load_model
from PIL import Image
def test(file):
    
    # Model loading
    svhn_model = load_model('svhn.h5')
#    svhn_model.summary()
    
    graph = Image.open(file)
    grapharr = np.array(graph)
    try:
        grapharr.shape == (32,32,3)
        graph_grey = rgb2gray(grapharr).astype(np.float32)
        graph_grey = graph_grey.reshape(1,32,32,1) # Reshape to feed to cnnmodel
        
        graph_pred = svhn_model.predict_classes(graph_grey, batch_size = 32, verbose = 1)
        
        return graph_pred[0]
        
    except:
        print ("Image size wrong, should be 32by32 pixels image")
        return None

#traintest()

#result1 = test("mytestimage1.png")
#print(result1)